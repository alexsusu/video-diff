"""
The original find_obj.py script was using SIFT/SURF/ORB to extract features
    from 2 images and applied the resulting homography transformation to nicely
    determine an object from image 1 to an image containing the object to
    image 2. However, the example they provided was highly... optimistic :))

We also added:
    Lucas-Kanade's image alignment/registration algorithm from lk_homography.py
    motion history and diffs
"""

#/usr/bin/env python

'''
Feature-based image matching sample.

USAGE
  find_obj.py [--feature=<sift|surf|orb>[-flann]] [ <image1> <image2> ]

  --feature  - Feature to use. Can be sift, surf of orb. Append '-flann' to feature name
                to use Flann-based matcher instead bruteforce.

  Press left mouse button on a feature point to see its matching point.
'''

#import cv
import cv2
import getopt
import numpy as np
import os
import sys
import time
import traceback
#print __doc__

import common
import common_cv
import config
import Clustering
import ECC
import Misc
import LK


FLANN_INDEX_KDTREE = 1  # bug: flann enums are missing
FLANN_INDEX_LSH    = 6



###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################

detector = None
matcher = None
numFramesQ = 0
numFramesR = 0
counterQ = -1
counterR = -1

kp1 = None
kp2 = None
desc1 = None
desc2 = None

imgQ = None
imgR = None

"""
Generated by FeatureMatchAndHomography() called by TemporalAlignment().
    Used in Rest().
"""
p1 = None
p2 = None
kp_pairs = None
status = None
H = None

"""
The keypoints matched by knnMatch() BUT FILTERED out by
    FilterKeypointsMatches() called by TemporalAlignment().
  nonp1 used in Rest().
"""
nonp1 = None
nonp2 = None

# Created by AnnotateVis
vis = None
visOrig = None

###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
def init_feature(name):
    chunks = name.split('-')

    """
    !!!!TODO: choose num of features to detect config.numFeaturesToExtractPerFrame
        depending on the resolution of the image
    """
    if chunks[0] == "sift":
        detector = cv2.SIFT()
        norm = cv2.NORM_L2
    elif chunks[0] == "surf":
        detector = cv2.SURF(config.numFeaturesToExtractPerFrame) #800)
        norm = cv2.NORM_L2
    elif chunks[0] == "orb":
        detector = cv2.ORB(config.numFeaturesToExtractPerFrame) #100) #400)
        #detector = cv2.ORB(1000)
        norm = cv2.NORM_HAMMING
    else:
        return None, None

    if "flann" in chunks:
        if norm == cv2.NORM_L2:
            flann_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        else:
            flann_params= dict(algorithm=FLANN_INDEX_LSH,
                               table_number=6, # 12
                               key_size=12,     # 20
                               multi_probe_level=1) #2
        matcher = cv2.FlannBasedMatcher(flann_params, {})  # bug : need to pass empty dict (#1329)
    else:
        matcher = cv2.BFMatcher(norm)

    return detector, matcher


def PreMain(nFramesQ, nFramesR):
    global detector, matcher
    global numFramesQ, numFramesR
    global kp1, kp2
    global desc1, desc2

    """
    # From http://docs.scipy.org/doc/numpy/reference/generated/numpy.set_printoptions.html
    numpy.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, suppress=None, nanstr=None, infstr=None, formatter=None)
        threshold : int, optional
            Total number of array elements which trigger summarization rather than full repr (default 1000).
        suppress : bool, optional
            Whether or not suppress printing of small floating point values using scientific notation (default False).
    """
    np.set_printoptions(threshold=1000000, linewidth=3000);

    numFramesQ = nFramesQ
    numFramesR = nFramesR

    feature_name = config.FEATURE_DETECTOR_AND_MATCHER

    common.DebugPrint("Alex: feature_name = %s" % feature_name)

    detector, matcher = init_feature(feature_name)
    if detector != None:
        print "using %s" % feature_name
    else:
        print "unknown feature: %s" % feature_name
        sys.exit(1)

    kp1 = [None] * numFramesQ
    kp2 = [None] * numFramesR
    desc1 = [None] * numFramesQ
    desc2 = [None] * numFramesR


###############################################################################
###############################################################################
#########################SPATIAL ALIGNMENT#####################################
###############################################################################
###############################################################################
###############################################################################

"""
SpatialAlignment() computes the homography, aligns and computes the warped
    image and returns vis with the input frame and the warped reference frame.
    Further processing of these frames is performed in AnnotateVis().

Note that status is obtained from computing the homography with
            cv2.findHomography() (from FeatureMatchAndHomography(),
                                called by TemporalAlignment()).

We annotate the visual (the pair of matched frames) with the detected
    features/keypoints and the meaningful clusters of non-matched features
    of the input frame.

Normally H is the homography obtained from the temporal alignment.
"""
def SpatialAlignment(win, inputFrame, refFrame, kp_pairs, status=None, H=None):
    hQ, wQ = inputFrame.shape[:2]
    hR, wR = refFrame.shape[:2]

    vis = None

    # Alex: if it's an inlier we draw a line, etc

    """
    Alex: we convert images to gray, otherwise we get exception like:
      Traceback (most recent call last):
        File "ReadAVI.py", line 321, in <module>
          Main()
        File "ReadAVI.py", line 258, in Main
          res = find_obj.ComputeFeatures2(refFrame, counterR)
        File "Z:\1PhD\UPB_RO\CV_Video_Based_Detection\1\find_obj.py", line 387, in ComputeFeatures2
          res = TemporalAlignment('find_obj')
        File "Z:\1PhD\UPB_RO\CV_Video_Based_Detection\1\find_obj.py", line 334, in TemporalAlignment
          vis = SpatialAlignment(win, inputFrame, refFrame, kp_pairs, status, H)
        File "Z:\1PhD\UPB_RO\CV_Video_Based_Detection\1\find_obj.py", line 161, in SpatialAlignment
          vis[:hQ, :wQ] = inputFrame
      ValueError: operands could not be broadcast together with shapes (576,720) (576,720,3)
    """

    """
    Note that the formal parameters of the caller corresponding to the actual
        parameters inputFrame and refFrame are not changed by the following assignments:
    """
    inputFrame = common.ConvertImgToGrayscale(inputFrame)
    refFrame = common.ConvertImgToGrayscale(refFrame)

    DISPLAY_WARPED_IMAGE = False

    if config.SPATIAL_ALIGNMENT_ALGO in ["LK", "TEMPORAL_ALIGNMENT_HOMOGRAPHY"]:
        if config.SPATIAL_ALIGNMENT_ALGO == "LK":
            # Computing homography with Lucas-Kanade's algorithm:
            H1, status1 = LK.LucasKanade_Homography(inputFrame)
            if H1 != None:
                # We take the H from the LK algorithm
                H = H1
                status = status1

        if config.USE_GUI or config.SAVE_FRAMES:
            # Applying overlay of warped refFrame with homography H
            h, w = refFrame.shape[:2]
            overlay = cv2.warpPerspective(src=refFrame, M=H, dsize=(w, h))
            #inputFrame = cv2.addWeighted(inputFrame, 0.5, overlay, 0.5, 0.0)

            vis = np.zeros((max(hQ, hR), wQ + wR), np.uint8)
            vis[:hQ, :wQ] = inputFrame

            if config.DISPLAY_RIGHT_IMAGE_WITH_DIFF:
                # refFrame is the warped refFrame, when applying H to it
                refFrame = overlay

                #!!!!TODO: using imgDiff = None seems to crash at absdiff(). This is just a crappy assignment, to be of the same type :))
                imgDiff = refFrame

                # Compute difference between frames inputFrame and refFrame:
                cv2.absdiff(src1=inputFrame, src2=refFrame, dst=imgDiff)
                vis[:hR, wQ : wQ + wR] = imgDiff
            else:
                if DISPLAY_WARPED_IMAGE:
                    refFrame = overlay

                vis[:hR, wQ : wQ + wR] = refFrame
    elif config.SPATIAL_ALIGNMENT_ALGO == "ECC":
        ECC.template_image = inputFrame
        ECC.target_image = refFrame
        ECC.number_of_iterations = 25 #7 #25
        ECC.termination_eps = 0.001
        ECC.motion = "homography"
        ECC.warp_to_file = "warp.ecc"
        ECC.warp_init = "init.txt"
        ECC.image_to_file = "warped.pgm"

        # Note that ECC.template_image is a numpy.ndarray

        """
        ECC.template_image = Misc.ConvertNPToCVMat(ECC.template_image)
        ECC.target_image = Misc.ConvertNPToCVMat(ECC.target_image)
        """
        ECC.target_image = Misc.ConvertNPToIplImage(ECC.target_image)
        ECC.template_image = Misc.ConvertNPToIplImage(ECC.template_image)

        if config.USE_GUI or config.SAVE_FRAMES:
            vis = np.zeros((max(hQ, hR), wQ + wR), np.uint8)
            vis[:hQ, :wQ] = inputFrame

            vis[:hR, wQ : wQ + wR] = refFrame #ECC.target_image
            if config.USE_GUI:
                cv2.imshow(win, vis)
                cv2.waitKey(2000) #10000)

        # We require IplImage; the result is basically ECC.warped_image
        ECC.main()

        if config.USE_GUI or config.SAVE_FRAMES:
            if False:
                refFrame = ECC.target_image

            #!!!!TODO: using imgDiff = None seems to crash at absdiff(). This is just a crappy assignment, to be of the same type :))
            imgDiff = refFrame #None #refFrame #None
            #imgDiff = ECC.warped_image

            refFrameNew = ECC.warped_image
            print "refFrameNew (before) = %s" % str(refFrameNew)

            """
            # We now need to convert refFrameNew to numpy:
              From https://stackoverflow.com/questions/13104161/fast-conversion-of-iplimage-to-numpy-array:

            "works fantastically, and more important: quickly!
            This is by far the fastest way I've found to grab a frame and make
                it an ndarray for numpy processing." - see code below:
            """
            #NOT GOOD: refFrameNew = np.asarray(refFrameNew) # Still is an IplImage()
            refFrameNew = np.asarray(refFrameNew[:, :])

            print "inputFrame = %s" % str(inputFrame)
            print "refFrame = %s" % str(refFrame)
            print "refFrameNew = %s" % str(refFrameNew)
            #print "imgDiff = %s" % str(imgDiff)

            """
            Because both inputFrame and refFrame are
                OLD:
                    inputFrame = <iplimage(nChannels=1 width=568 height=320 widthStep=568 )>
                    imgDiff = <iplimage(nChannels=1 width=568 height=320 widthStep=568 )>
                    **
                    inputFrame = <iplimage(nChannels=1 width=568 height=320 widthStep=568 )>
                    refFrame = [[47 46 47 ..., 59 49 39]
                     [47 47 46 ..., 61 49 39]
                     [46 47 46 ..., 61 49 38]
                     ...,
                     [85 85 86 ..., 79 79 79]
                     [86 86 88 ..., 82 82 82]
                     [88 88 88 ..., 84 83 84]]
                **
                inputFrame = <iplimage(nChannels=1 width=568 height=320 widthStep=568 )>
                refFrame = <iplimage(nChannels=1 width=568 height=320 widthStep=568 )>

            We have the following error:
                "TypeError: src1 is not a numpy array, neither a scalar"
            """
            if config.DISPLAY_RIGHT_IMAGE_WITH_DIFF:
                # Compute difference between frames inputFrame and refFrame:
                cv2.absdiff(src1=inputFrame, src2=refFrameNew, dst=imgDiff)
            else:
                imgDiff = refFrameNew

            """
            We display in the right pane the diff between the warped image of
                the target image and the template?
            """
            vis[:hR, wQ : wQ + wR] = imgDiff

    """
    !!!!TODO: return reduced_feature_set - feature_set keypoints of input frame
        that overlap with ref_frame.

    #    !!!!TODO: in many cases the corners after applying H are just 1 point or a line or a very narrow quadrilater - also the feature points
    # We now draw the feature points of the INPUT frame on which we apply H
    ft = np.float32([kpp[1].pt for kpp in kp_pairs]) #!!!!TODO: should be kpp[0].pt, NOT the features of the reference frame
    print "ft = %s" % str(ft)
    ftT = cv2.perspectiveTransform(ft.reshape(1, -1, 2), H)
    print "ftT = %s" % str(ftT)
    ft = np.int32(ftT.reshape(-1, 2) + (wQ, 0))
    print "ft (after) = %s" % str(ft)
    for e in ft:
        if keypoint (e[0], e[1])  inside the homography of the rectangle of the frame then the keypoint is in reduced_feature_set
    """
    return (vis, inputFrame, refFrame, status)


###############################################################################
###############################################################################
###############################################################################
#########################END SPATIAL ALIGNMENT#################################
###############################################################################
###############################################################################
###############################################################################

def ClusterUnmatchedKeypoints(Z):
    #X = np.random.randint(25,50,(25,2))
    #Y = np.random.randint(60,85,(25,2))
    #Z = np.vstack((X, Y))

    # Start time profiling for the inner loop
    t1 = float(cv2.getTickCount())

    #print "Z =", Z
    Z = np.array(Z)
    # convert to np.float32
    Z = np.float32(Z)

    N = len(Z) # the number of points
    common.DebugPrint("N = %d" % N)

    #if False:
    if True:
        #!!!!TODO: should we cluster also the non-matched features of the reference video (B) ?
        #!!!!TODO TODO_IMPORTANT: more important: now Hierarchical Clustering returns the clusters - you can color them differently
        Z = Clustering.HierarchicalClustering(Z, N)
    else:
        Clustering.HierarchicalClusteringWithCV2_UNFINISHED(Z, N)

    t2 = float(cv2.getTickCount())
    myTime = (t2 - t1) / cv2.getTickFrequency()

    common.DebugPrint(
        "ClusterUnmatchedKeypoints(): HierarchicalClustering() " \
        "took %.5f [sec]" % myTime)

    return Z

###############################################################################
###############################################################################
##############################END CLUSTERING###################################
###############################################################################
###############################################################################
###############################################################################

# inputFrame, refFrame are required basically to get again the frame dimensions
def AnnotateVis(win, vis, inputFrame, refFrame, statusLocal):
    global visOrig
    """
    global vis, vis0, winGlobal, p1, p2, statusGlobal
    winGlobal = win
    """

    visOrig = vis.copy()
    vis = cv2.cvtColor(vis, cv2.COLOR_GRAY2BGR)

    hQ, wQ = inputFrame.shape[:2]
    hR, wR = refFrame.shape[:2]

    blue = (255, 0, 0)
    pink = (255, 128, 255)
    white = (255, 255, 255)

    kpColor = (51, 103, 236)
    red = (0, 0, 255)
    green = (0, 255, 0)
    GREEN_GRAY = (51, 173, 136)

    if config.USE_GUI or config.SAVE_FRAMES:
        ##########################DRAW THE KEYPOINTS###########################
        ##########################DRAW THE KEYPOINTS###########################
        ##########################DRAW THE KEYPOINTS###########################
        ##########################DRAW THE KEYPOINTS###########################
        #if False:
        #if True:
        # Not VERY necessary to represent them

        # Alex: we represent also ALL THE features of the 2 images:
        for e in kp1[counterQ]:
            #cv2.circle(vis, (int(e.pt[0]), int(e.pt[1])), 2, blue, -1)
            cv2.circle(vis, (int(e.pt[0]), int(e.pt[1])), 10, blue, -1)

        # We draw for image2, hence + wQ translation on horizontal
        for e in kp2[counterR]:
            #print "e =", e
            #cv2.circle(vis, (int(e.pt[0]) + int(wQ), int(e.pt[1])), 2, pink, -1)
            cv2.circle(vis, (int(e.pt[0] + wQ), int(e.pt[1])), 10, pink, -1)
            #cv2.circle(vis, (e[0], e[1]), 2, pink, -1)
        """
        # Alex: we represent also all the matched features of the 2 images:
        for e in ftImg1:
            cv2.circle(vis, (e[0], e[1]), 4, blue, -1)
        # We draw for image2, hence + wQ translation on horizontal
        for e in ftImg2:
            #print "e =", e
            cv2.circle(vis, (int(e[0]) + int(wQ), int(e[1])), 5, pink, -1)
            #cv2.circle(vis, (e[0], e[1]), 2, pink, -1)
        """

        """
        Note: nonp1 is updated by Clustering.HierarchicalClustering() - it
            represents the non-matched features that form a dense
            cluster of more than THRESHOLD_NUM_NONMATCHED_ELEMENTS_IN_CLUSTER
            elements.

        We draw the NON-matched features for frame of video A:
        """
        for e in nonp1:
            cv2.circle(vis, (int(e[0]), int(e[1])), 7, GREEN_GRAY, -1)
            #cv2.circle(vis, (int(e[0]), int(e[1])), 7, (255, 253, 255), -1)

        # We draw the NON-matched features for frame of video B:
        for e in nonp2:
            cv2.circle(vis, (int(e[0] + wQ), int(e[1])), 7, GREEN_GRAY, -1)
            #cv2.circle(vis, (int(e[0]), int(e[1])), 7, (255, 253, 255), -1)

        ##########################END DRAW THE KEYPOINTS#######################
        ##########################END DRAW THE KEYPOINTS#######################
        ##########################END DRAW THE KEYPOINTS#######################
        ##########################END DRAW THE KEYPOINTS#######################

    if config.USE_GUI or config.SAVE_FRAMES:
        """
        We draw the homography transformation by looking at the identified
            features - H is basically a rotation and zoom (transformation in
            homogenous?? coordinates).
        """
        if H is not None:
            corners = [[0, 0], [wQ, 0], [wQ, hQ], [0, hQ]]
            print "corners = %s" % str(corners)

            #if True:
            if False:
                #corners = [[20, 20], [wQ-10, 20], [wQ-10, hQ-10], [20, hQ-10]]
                corners = np.int32(corners)
                cv2.polylines(img=vis, pts=[corners], isClosed=True, \
                                    color=(255, 255, 255))

            corners = np.float32(corners)

            # See http://stackoverflow.com/questions/6627647/reshaping-a-numpy-array-in-python for description of numpy.reshape()
            try:
                cornersT = cv2.perspectiveTransform(src=corners.reshape(1, -1, 2), m=H)
                print "cornersT = %s" % str(cornersT)
                corners = np.int32(cornersT.reshape(-1, 2) + (wQ, 0))
                print "corners = %s" % str(corners)
            except:
                common.DebugPrintErrorTrace()
                common.DebugPrint("!!!!!!!!exception at perspectiveTransform(): H=%s" % \
                                            str(H))
                corners = np.int32(corners.reshape(-1, 2) + (wQ, 0))

            """
            print "cornersT = %s" % str(cornersT)
            corners = np.int32(cornersT.reshape(-1, 2) + (wQ, 0))
            print "corners = %s" % str(corners)
            """
            cv2.polylines(img=vis, pts=[corners], isClosed=True, \
                                        color=(255, 255, 255))

        if False:
            #!!!!TODO: in many cases the corners after applying H are just 1 point or a line or a very narrow quadrilater - also the feature points
            # We now draw the feature points of the INPUT frame on which we apply H
            ft = np.float32([kpp[1].pt for kpp in kp_pairs]) #!!!!TODO: should be kpp[0].pt, NOT the features of the reference frame
            print "ft = %s" % str(ft)
            ftT = cv2.perspectiveTransform(ft.reshape(1, -1, 2), H)
            print "ftT = %s" % str(ftT)
            ft = np.int32(ftT.reshape(-1, 2) + (wQ, 0))
            print "ft (after) = %s" % str(ft)
            for e in ft:
                cv2.circle(vis, (int(e[0]), int(e[1])), 10, (255, 255, 255), -1)

    if config.USE_GUI:
        if statusLocal is None:
            statusLocal = np.ones(len(kp_pairs), np.bool_)

        if kp_pairs == []:
            p1Local = []
            p2Local = []
        else:
            p1Local = np.int32([kpp[0].pt for kpp in kp_pairs])
            p2Local = np.int32([kpp[1].pt for kpp in kp_pairs]) + (wQ, 0)

        common.DebugPrint("len(p1Local) = %d" % \
                                                (len(p1Local)))
        common.DebugPrint("len(kp_pairs) = %d" % \
                                                (len(kp_pairs)))

        for (x1, y1), (x2, y2), inlier in zip(p1Local, p2Local, statusLocal):
            if inlier:
                col = green
                #cv2.circle(vis, (x1, y1), 2, col, -1)
                #cv2.circle(vis, (x2, y2), 2, col, -1)
                cv2.circle(vis, (x1, y1), 5, col, -1)

                if False:
                    cv2.circle(vis, (x2, y2), 5, col, -1)
                else:
                    cv2.circle(vis, (x2, y2), 8, col, -1)
            else:
                col = red
                r = 2
                thickness = 4 #10 #3
                # We draw some sort of squares for the
                cv2.line(vis, (x1-r, y1-r), (x1+r, y1+r), col, thickness)
                cv2.line(vis, (x1-r, y1+r), (x1+r, y1-r), col, thickness)
                cv2.line(vis, (x2-r, y2-r), (x2+r, y2+r), col, thickness)
                cv2.line(vis, (x2-r, y2+r), (x2+r, y2-r), col, thickness)

        """
          They use it when clicking on mouse to make the corresponding lines
            disappear between the 2 images.
        """
        if False:
            vis0 = vis.copy()
        else:
            vis0 = visOrig.copy()

        # This is where we draw the green lines between the 2 images
        for (x1, y1), (x2, y2), inlier in zip(p1Local, p2Local, statusLocal):
            if inlier:
                cv2.line(vis, (x1, y1), (x2, y2), green, 2)

        cv2.imshow(win, vis)

        """
        # Used if we make onmouse() a global function
        statusGlobal = statusLocal
        """

        """
        IMPORTANT: a limitation of Python 2.x is that inner functions
            do a symboltable lookup of non-local variables in the global
            scope not the immediately outter scope of the function - in
            this case the one of AnnotateVis().
          This is why we differentiate variables with the same name:
            statusLocal and statusGlobal
            p1Local and p1 .
        """
        def onmouse(event, x, y, flags, param):
            crtVis = vis
            if flags & cv2.EVENT_FLAG_LBUTTON:
                crtVis = vis0.copy()
                r = 3 #8

                if False:
                    common.DebugPrint("onmouse(): x = %d, y = %d" % (x, y))
                    test = p1Local - (x, y)
                    common.DebugPrint("onmouse(): test = %s" % str(test))

                m = (common_cv.anorm(p1Local - (x, y)) < r) | \
                    (common_cv.anorm(p2Local - (x, y)) < r)
                idxs = np.where(m)[0]
                kp1s, kp2s = [], []

                assert len(p1Local) == len(p2Local)
                if False:
                    common.DebugPrint("onmouse(): len(p1Local) = %d, len(m) = %d" % \
                                                        (len(p1Local), len(m)))
                    common.DebugPrint("onmouse(): len(kp_pairs) = %d" % \
                                                        (len(kp_pairs)))
                    common.DebugPrint("onmouse(): m = %s" % str(m))
                    common.DebugPrint("onmouse(): idxs = %s" % str(idxs))

                for i in idxs:
                    (x1, y1), (x2, y2) = p1Local[i], p2Local[i]

                    if False:
                        common.DebugPrint("onmouse(): m[%d] = %s" % (i, str(m[i])))
                        common.DebugPrint("onmouse():  p1Local[%d] = %s" % (i, str(p1Local[i])))
                        common.DebugPrint("onmouse():  p2Local[%d] = %s" % (i, str(p2Local[i])))
                        common.DebugPrint("onmouse():  kp_pairs[%d] = %s %s" % \
                                            (i, str(kp_pairs[i][0].pt), str(kp_pairs[i][1].pt) ))

                    try:
                        col = (red, green)[statusLocal[i]]
                    except:
                        common.DebugPrintErrorTrace()
                        common.DebugPrint("onmouse() exception: i = %d, " \
                                "len(statusLocal) = %d" % (i, len(statusLocal)))
                        col = red

                    cv2.line(crtVis, (x1, y1), (x2, y2), col)

                    try:
                        kp1, kp2 = kp_pairs[i]
                        kp1s.append(kp1)
                        kp2s.append(kp2)
                    except:
                        common.DebugPrintErrorTrace()
                        common.DebugPrint("onmouse() exception2: i = %d, " \
                                "len(kp_pairs)=%d, len(idxs)=%d, idxs=%s" % \
                                (i, len(kp_pairs), len(idxs), str(idxs)))

                crtVis = cv2.drawKeypoints(crtVis, kp1s, flags=4, \
                                                        color=kpColor)
                crtVis[:, wQ:] = cv2.drawKeypoints(crtVis[:, wQ:], kp2s, \
                                                    flags=4, color=kpColor)

            cv2.imshow(win, crtVis)
        #######################################################################
        #######################################################################
        #######################################################################

        cv2.setMouseCallback(win, onmouse)

    """
    We now save the processed frames (not the original one)
        - we show the features and the new elements in the image
    """
    if False:
        if config.SAVE_FRAMES:
            #print "dir(refFrame) = %s"% str(dir(refFrame))
            if False:
                """
                refFrameCV = cv.fromarray(refFrame)
                cv2.imwrite("img_proc_%05d_%05d.png" % (counterQ, counterR),
                                                                    refFrameCV)
                """
                cv2.imwrite("img_proc_%05d_%05d.png" % (counterQ, counterR),
                                                                    refFrame)
            """
            visCV = cv.fromarray(vis)
            cv2.imwrite(config.FRAME_PAIRS_FOLDER + "/img_proc_%05d_%05d.png" % \
                            (counterQ, counterR), visCV)
            """
            cv2.imwrite(config.FRAME_PAIRS_FOLDER + "/img_proc_%05d_%05d.png" % \
                            (counterQ, counterR), vis)

    return vis


#def SpatialAlignmentAndClustering()
#def Rest(win, imgQ, imgR, nonp1, kp_pairs, status, H, p1, p2): #, (counterQ, ) counterR):
def Rest(win):
    global nonp1
    global status

    # Here we perform spatial alignment and clustering and display the results.
    t1 = float(cv2.getTickCount())

    #!!!!TODO: filter out p1 outside the alignment of the pair of frames
    (myVis, inputFrame, refFrame, status2) = SpatialAlignment(win, imgQ, imgR, \
                                                            kp_pairs, status, H) # , nonp1

    t2 = float(cv2.getTickCount())
    myTime = (t2 - t1) / cv2.getTickFrequency()
    common.DebugPrint(
        "FeatureMatchAndHomography(): SpatialAlignment() took %.6f [sec]" % \
        (myTime))

    #!!!!TODO: filter out nonp1 outside the alignment of the pair of frames - should do it on p1 inside SpatialAlignment()
    # We cluster the non-matched keypoints
    nonp1 = ClusterUnmatchedKeypoints(nonp1)
    #nonp1 = nonp1[:3]

    #myVis2 = AnnotateVis(win, myVis, inputFrame, refFrame, status2) # This seems to cause "IndexError: index out of bounds exceptions" in onmouse()
    myVis2 = AnnotateVis(win, myVis, inputFrame, refFrame, status)

    #!!!!TODO: remove global definition of vis
    global vis
    vis = myVis2

    if config.USE_GUI:
        #if False:
        if True:
            #cv2.waitKey()
            #ch = cv2.waitKey(1) # Can return -1 which means no key pressed
            ch = cv2.waitKey()
            print "ch = %s" % str(ch)
            if ch == 27: # Escape key
                quit()

            """
            ch = cv2.waitKey(1)
            if ch == ord(' '):
                self.paused = not self.paused
            if ch == ord('c'):
                self.tracker.clear()
            if ch == 27:
                break
            """
        else:
            #time.sleep(3.0)
            cv2.waitKey(2000) #milliseconds - see Section 4.1

        cv2.destroyAllWindows()


###############################################################################
###############################################################################
###############################################################################
#############################TEMPORAL ALIGNMENT################################
###############################################################################
###############################################################################
###############################################################################
def ComputeFeatures1(aImg1, aCounterQ):
    global kp1, desc1
    global counterQ
    global imgQ

    imgQ = aImg1

    counterQ = aCounterQ

    common.DebugPrint("ComputeFeatures1(): counterQ = %d" % counterQ)

    #print "ComputeFeatures1(): kp1 = %s" % str(kp1)
    if kp1[counterQ] == None:
        t1 = float(cv2.getTickCount())

        kp1[counterQ], desc1[counterQ] = detector.detectAndCompute(aImg1, None)

        t2 = float(cv2.getTickCount())
        myTime = (t2 - t1) / cv2.getTickFrequency()
        common.DebugPrint(
            "ComputeFeatures1(): detector.detectAndCompute() took %.6f [sec]" % \
            (myTime))

    #if True:
    if False:
        print "kp1[counterQ] = %s" % str(kp1[counterQ])
        print "desc1[counterQ] = %s" % str(desc1[counterQ])
        print "aImg1 - %d descriptors" % (len(desc1[counterQ]))

        print "aImg1 - kp1[counterQ][0]:"
        print "    pt = %s" % (str(kp1[counterQ][0].pt))
        print "    size = %s" % (str(kp1[counterQ][0].size))
        print "    angle = %s" % (str(kp1[counterQ][0].angle))
        print "    response = %s" % (str(kp1[counterQ][0].response))
        print "    octave = %s" % (str(kp1[counterQ][0].octave))
        print "    class_id = %s" % (str(kp1[counterQ][0].class_id))
        print "aImg1 - desc1[counterQ][0] = %s" % (str(desc1[counterQ][0]))

    common.DebugPrint("aImg1 - %d features" % len(kp1[counterQ]))


def ComputeFeaturesAndMatch2(aImgR, aCounterR):
    global kp2, desc2
    global counterR
    global imgR

    imgR = aImgR

    #common.DebugPrint("ComputeFeaturesAndMatch2(): aImgR = %s" % str(aImgR))

    counterR = aCounterR

    if counterR == None:
        counterR = 0

    common.DebugPrint("ComputeFeaturesAndMatch2(): counterR = %d (counterQ = %d)" % \
                                                        (counterR, counterQ))

    # If we did NOT memoize kp2[counterR], desc2[counterR] we compute them once
    if kp2[counterR] == None:
        """
        From http://docs.opencv.org/trunk/modules/nonfree/doc/feature_detection.html
            "Python API provides three functions. First one finds keypoints only.
            Second function computes the descriptors based on the keypoints we provide.
            Third function detects the keypoints and computes their descriptors.
            If you want both keypoints and descriptors, directly use third
                function as kp, des = cv2.SIFT.detectAndCompute(image, None)"
        """
        t1 = float(cv2.getTickCount())

        kp2[counterR], desc2[counterR] = detector.detectAndCompute(aImgR, None)

        t2 = float(cv2.getTickCount())
        myTime = (t2 - t1) / cv2.getTickFrequency()
        common.DebugPrint(
            "ComputeFeaturesAndMatch2(): detector.detectAndCompute() took %.6f [sec]" % \
            (myTime))

    common.DebugPrint("aImgR - %d features" % len(kp2[counterR]))

    res = FeatureMatchAndHomography()

    """
    The result is related to the homography transformation returned by
            FeatureMatchAndHomography(): status sum and len
    """
    return res
    #return TemporalAlignment("Image Match")



myTemporalAlignmentCost = 0
"""
FilterKeypointsMatches() has a time complexity of O(len(matches))
Note: 0.99 is a very permissive thershold and some matches are ~bogus
"""
def FilterKeypointsMatches(kp1, kp2, matches, ratio=0.95): #0.85): #0.99): #0.75):
    global nonp1, nonp2
    global myTemporalAlignmentCost

    myTemporalAlignmentCost = 0

    mkp1, mkp2 = [], []
    nonp1, nonp2 = [], []

    index = 0
    for m in matches:
        len_m = len(m)
        if len_m == 2:
            if m[0].distance < m[1].distance * ratio:
                """
                Inspired from
                    http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf#page=20:
                  "A more effective measure is obtained by comparing the
                    distance of the closest neighbor to that of the
                    second-closest neighbor."
                  Note:
                    m[0] = closest neighbor match
                    m[1] = 2nd-closest neighbor match
                """

                # These keypoints are considered matched - we choose the 1st closest match

                #if True:
                if False:
                    print "m[0].queryIdx = %s" % str(m[0].queryIdx)
                    print "m[0].trainIdx = %s" % str(m[0].trainIdx)
                    print "m[0].distance= %s" % str(m[0].distance)

                    print "m[1].queryIdx = %s" % str(m[1].queryIdx)
                    print "m[1].trainIdx = %s" % str(m[1].trainIdx)
                    print "m[1].distance = %s" % str(m[1].distance)
                    #assert m[0].queryIdx == m[1].trainIdx
                    #assert m[1].queryIdx == m[0].trainIdx

                mkp1.append( kp1[m[0].queryIdx] )
                mkp2.append( kp2[m[0].trainIdx] )

                myTemporalAlignmentCost += m[0].distance
            else:
                # These keypoints are considered NOT matched
                nonp1.append(kp1[m[0].queryIdx].pt)

                #!!!!TODO: I think that nonp2.append below is NOT relevant
                #nonp2.append(kp2[m[0].trainIdx].pt)
        else:
            assert len_m <= 2

            # These keypoints are considered NOT matched
            if len_m == 1:
                if False:
                    print "FilterKeypointsMatches(): adding feature with only %d matches!!" % len_m
                nonp1.append(kp1[m[0].queryIdx].pt)
            else:
                if False:
                    print "FilterKeypointsMatches(): found an elem with %d matches (at index = %d)!!" % (len_m, index)
                nonp1.append(kp1[index].pt)
        index += 1

    """
    nonp1 is a list of pairs of x, y coordinates for the features/keypoints
        that have not been matched from frame of video A.
    """
    common.DebugPrint("len(nonp1) = %d" % len(nonp1))
    common.DebugPrint("len(nonp2) = %d" % len(nonp2))

    #if True:
    if False:
        common.DebugPrint("nonp1 = %s" % str(nonp1))
        common.DebugPrint("nonp2 = %s" % str(nonp2))

    p1 = np.float32([kp.pt for kp in mkp1])
    p2 = np.float32([kp.pt for kp in mkp2])
    kp_pairs = zip(mkp1, mkp2)

    #if True:
    if False:
        print "FilterKeypointsMatches(): matches = %s" % str(matches)
        print "FilterKeypointsMatches(): p1 = %s" % str(p1)
        print "FilterKeypointsMatches(): p2 = %s" % str(p2)
        print "FilterKeypointsMatches(): kp_pairs = %s" % str(kp_pairs)

    return p1, p2, kp_pairs




"""
Part of temporal alignment.

TODO: We currently use a ~Lazy evaluation might not be best for cache (I-cache and D-cache).
    I guess the best is to have first preprocessed completely video 1,
      then preprocessed video2 then do a match between the 2 videos.
"""

#def FeatureMatchAndHomography(win):
def FeatureMatchAndHomography():
    #!!!!TODO: memoize (but it's gonna be huge) the pair results of matching - should reduce by 2 the runtime
    #global nonp1, nonp2
    global p1, p2, kp_pairs, status, H

    print "Entered FeatureMatchAndHomography()..."

    # rawMatches still has the same len as desc1[counterQ] AND trainDescriptors

    t1 = float(cv2.getTickCount())

    """
    opencv2refman.pdf, Section 7.4., page 429:
      knnMatch
        "Finds the k best matches for each descriptor from a query set."
        "k - Count of best matches found per each query descriptor or less if a
            query descriptor has less than k possible matches in total."
    """
    rawMatches = matcher.knnMatch(desc1[counterQ],
                                    trainDescriptors=desc2[counterR], k=2) #2

    p1, p2, kp_pairs = FilterKeypointsMatches(kp1[counterQ], kp2[counterR],
                                                                rawMatches)

    t2 = float(cv2.getTickCount())
    myTime = (t2 - t1) / cv2.getTickFrequency()
    common.DebugPrint(
        "FeatureMatchAndHomography(): knnMatch() and FilterKeypointsMatches() " \
        "took %.6f [sec]" % (myTime))

    """
    # We cluster & display the non-matched keypoints
    nonp1 = ClusterUnmatchedKeypoints(nonp1)
    """

    """
    rawMatches is a list of corresponding?? pairs of DMatch for the keypoints
        obtained with knnMatch() .

    p1, p2 are only the matched keypoints from the 2 frames

    kp_pairs is pair of corresponding keypoints, obtained with the list zip operation:
            cardinality ~=???? the cardinality of kp1[counterQ] and kp2[counterR]
    """

    #if True:
    if False:
        try:
            #print "len(kp1) = %d" % len(kp1)
            #print "len(kp2) = %d" % len(kp2)
            print "len(kp1[counterQ]) = %d" % len(kp1[counterQ])
            print "len(kp2[counterR]) = %d" % len(kp2[counterR])
            #
            print "len(p1) = %d" % len(p1)
            print "len(p2) = %d" % len(p2)
            print "len(kp_pairs) = %d" % len(kp_pairs)
            print "len(rawMatches) = %d" % len(rawMatches)
            #
            print "p1 = %s" % str(p1)
            print "p2 = %s" % str(p2)
            print "kp_pairs = %s" % str(kp_pairs)
            print "rawMatches[0] = %s" % str(rawMatches[0])
            #Still prints just a DMatch without extra info: print "rawMatches[0][0] = %s" % str(rawMatches[0][0])
            print "rawMatches[0][0].queryIdx = %s" % str(rawMatches[0][0].queryIdx)
            print "rawMatches[0][0].trainIdx = %s" % str(rawMatches[0][0].trainIdx)
            print "rawMatches[0][0].imgIdx = %s" % str(rawMatches[0][0].imgIdx)
            print "rawMatches = %s" % str(rawMatches)

            #if False:
            print "kp1[counterQ] = %s" % str(kp1[counterQ])
            print "desc1[counterQ] = %s" % str(desc1[counterQ])
            print "imgQ - %d descriptors" % (len(desc1[counterQ]))

            print "imgQ - kp1[counterQ][0].pt = %s" % (str(kp1[counterQ][0].pt))
            print "imgQ - desc1[counterQ][0] = %s" % (str(desc1[counterQ][0]))
        except:
            common.DebugPrintErrorTrace()

    if len(p1) >= 4:
        # p1 and p2 are the matched keypoints

        """
        From Section 6.1, page 391, opencv2refman.pdf:
            (also http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=findhomography#findhomography):
              "Finds a perspective transformation between two planes."
              "However, if not all of the point pairs (srcPoints
              i ,:math:dstPoints_i ) fit the rigid perspective transformation (that is,
              there are some outliers), this initial estimate will be poor."

          "In this case, you can use one of the two robust methods.
          Both methods, RANSAC and LMeDS , try many different random subsets
            of the corresponding point pairs (of four pairs each), estimate
            the homography matrix using this subset and a simple least-square
            algorithm, and then compute the quality/goodness of the computed
            homography (which is the number of inliers for RANSAC or the median
            re-projection error for LMeDs). The best subset is then used to
            produce the initial estimate of the homography matrix and the
            mask of inliers/outliers."
        """
        t1 = float(cv2.getTickCount())

        #cv2.findHomography(srcPoints, dstPoints[, method[, ransacReprojThreshold[, mask ]]]) --> ret-val, mask
        H, status = cv2.findHomography(srcPoints=p1, dstPoints=p2, \
                        method=cv2.RANSAC, ransacReprojThreshold=5.0) #, mask=3.0)

        t2 = float(cv2.getTickCount())
        myTime = (t2 - t1) / cv2.getTickFrequency()
        common.DebugPrint(
            "FeatureMatchAndHomography(): cv2.findHomography() took %.6f [sec]" % \
            (myTime))

        #if True:
        if False:
            print "status = %s" % (str(status))

        #assert status != None #This assertion is violated (at least on OpenCV 3.0)
        if status == None:
            status = []

        if H == None:
            print "!!!!!!!!found H None - len(p1) = %d" % (len(p1))
            H = []
        return (-1, len(p1))

        print "%d / %d  inliers/matched (len(p1) = %d)" % (np.sum(status), len(status), len(p1))

        # Note: H is the homography matrix, a 3x3 matrix.
        common.DebugPrint(
            "H, the homography matrix, from cv2.findHomography = %s" % str(H))

        common.DebugPrint("     len(H) = %d" % len(H))

        # Note that len(status) == len(p1)
        res = (np.sum(status), len(status))
    else:
        H, status = None, None
        common.DebugPrint(
            "%d matches found, not enough for homography estimation" % len(p1))

        res = (-1, len(p1))

    # The result is related to the homography transformation: status sum and len
    return res


"""
Avoid using so many globals.

Should we have modules:
    TemporalAlignment
    SpatialAlignment
    Clustering
  ?


!!!!TODO !!!!!!!! Do better interface for the processing pipeline:
    e.g., TODO
      - implement return (frame, ref_frame, feature_set, ref_feature_set)
         (frame, ref_frame, feature_set, ref_feature_set)
           - frame este un frame din filmul curent
           - ref_frame este frame-ul cel mai apropiat din filmul referinta
           - feature_set este multimea descriptorilor SIFT/ORB/whatever extrasi din frame
           - ref_feature_set este multimea descriptorilor SIFT/ORB/whatever extrasi din ref_frame

      - pasul de spatial aligment urmareste sa identificam sectiunile
        din frame care nu se regasesc in ref_frame, si sa eliminam din feature_set
        acele features care cad in afara zonei de suprapunere intre frame si
        ref_frame.

         Pasul de aliniere spatiala livreaza catre clustering un tuplu (frame,
          reduced_feature_set), unde:
          - frame este un cadru din filmul curent (nici o modificare fata de ce am
                      primit de la alinierea temporala)
          - reduced_feature_set este submultimea lui feature_set care indeplineste
              conditia ca feature-ul se afla intr-o zona de suprapunere a lui
              frame cu ref_frame

      - pasul Clustering livreaza urmatoarele: (frame, cluster_areas), unde:
        - frame iarasi e nemodificat
        - cluster_areas reprezinta definitiile spatiale ale clusterelor identificate

Currently the Rest() of the pipepline is called by TemporalAlignment, when
    found an optimal candidate.

Temporal alignment:
    - for each reference frame:
        - we also detect features (and compute descriptors) for reference frame
        - FeatureMatchAndHomography()
            - knnMatch()
            - FilterKeypointsMatches()
            - homography
"""
def TemporalAlignment(counterQ, frameQ, captureR, numFramesR, \
                                    numFeaturesMatched, fOutput):
    global p1, p2, kp_pairs, status, H, nonp1

    if config.USE_EXHAUSTIVE_SEARCH:
        maxFeaturesMatched = -2000000000 #-1
        posMaxFeaturesMatched = -1

        while True:
            if config.OCV_OLD_PY_BINDINGS:
                frameR = captureR.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)
            else:
                frameR = captureR.get(cv2.CAP_PROP_POS_FRAMES)
            common.DebugPrint("Alex: frameR = %d" % frameR)

            counterR = int(frameR) #0

            ret2, imgR = captureR.read()
            if (ret2 == False) or (counterR > numFramesR):
                break

            #if False and config.SAVE_FRAMES:
            if config.SAVE_FRAMES:
                fileName = config.IMAGES_FOLDER + "/imgR_%05d.png" % counterR
                if not os.path.exists(fileName):
                    #print "dir(imgR) = %s"% str(dir(imgR))
                    """
                    imgRCV = cv.fromarray(imgR)
                    cv2.imwrite(fileName, imgRCV)
                    """
                    cv2.imwrite(fileName, imgR)
                    #quit()

            if False:
            #if True:
                print "Alex: ret1 = %s" % (str(ret1))
                print "Alex: imgQ = %s" % (str(imgQ))

                print "Alex: ret2 = %s" % (str(ret2))
                print "Alex: imgR = %s" % (str(imgR))

            if False:
                cv2.imshow("imgQ", imgQ)
                cv2.imshow("imgR", imgR)

            if False:
                #result = processFrame(img)
                result = img
                cv2.imshow("some", result)
                if 0xFF & (cv2.waitKey(5) == 27):
                    break

            """
            I don't need to change to gray image if I do NOT do
                MatchFrames....() , which requires gray to
                concatenate the 2 frames together.
            """
            if False:
            #if True:
                """
                Note: imgQ and imgR are RGB-images.
                    I need to convert them to grayscale
                        (since cv2.imread(fn1, 0) reads grayscale)
                """
                imgR = common.ConvertImgToGrayscale(imgR)

            res = ComputeFeaturesAndMatch2(imgR, counterR)
            # res[1] = number of matched features

            COST_USED = 1
            if COST_USED == 0:
                pass
            else:
                # myTemporalAlignmentCost is the sum of distances of best-pairs (closest neighbors) of matched features
                res = (res[0], -myTemporalAlignmentCost)
                #res = (res[0], -myTemporalAlignmentCost / res[1])

            numFeaturesMatched[counterQ][counterR] = res[1]

            COMPUTE_BEST_FAST = False #True

            if maxFeaturesMatched < res[1]:
                maxFeaturesMatched = res[1]
                posMaxFeaturesMatched = counterR

                if config.SAVE_FRAMES:
                    if COMPUTE_BEST_FAST == False:
                        """
                        #!!!!TODO: don't do it even for best candidates - only for the best ONE - this implies redoing probably some computation from TemporalAlignment() for the best frame pair
                        We call Rest() in order to compute vis and visOrig.
                        """
                        Rest("Image Match")

                        visBest = vis.copy()
                        visOrigBest = visOrig.copy()
                    else:
                        p1Best = p1
                        p2Best = p2
                        kp_pairsBest = kp_pairs
                        statusBest = status
                        HBest = H
                        nonp1Best = nonp1

                    #if True:
                    if False:
                        #res = ComputeFeaturesAndMatch2(imgR, counterR) # We repeat the MatchFrame to save the frames

                        """
                        visCV = cv.fromarray(vis)
                        cv2.imwrite(config.FRAME_PAIRS_MATCHES_FOLDER + \
                                        "/img_proc_%05d_%05d.png" % \
                                        (counterQ, counterR), visCV)
                        """
                        cv2.imwrite(config.FRAME_PAIRS_MATCHES_FOLDER + \
                                        "/img_proc_%05d_%05d.png" % \
                                        (counterQ, counterR), vis)

                        """
                        visCV = cv.fromarray(visBest)
                        cv2.imwrite(config.FRAME_PAIRS_MATCHES_FOLDER + \
                                        "/1img_proc_%05d_%05d.png" % \
                                        (counterQ, counterR), visCV)
                        """
                        cv2.imwrite(config.FRAME_PAIRS_MATCHES_FOLDER + \
                                        "/1img_proc_%05d_%05d.png" % \
                                        (counterQ, counterR), vis)

            common.DebugPrint("Alex: counterR = %d" % counterR)

            counterR += config.counterRStep
            if False:
                counterQ += config.counterQStep #10 #1

            """
            If we try to seek to a frame out-of-bounds frame it gets to
                the last one.
            """
            if config.OCV_OLD_PY_BINDINGS:
                captureR.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, counterR)
            else:
                captureR.set(cv2.CAP_PROP_POS_FRAMES, counterR)

            common.DebugPrint("Alex: Time = %s" % \
                            common.GetCurrentDateTimeStringWithMilliseconds())

        myText = "  Frame #%d of video A: " \
            "frame #%d of video B, with %d features matched (time = %s)" % \
            (counterQ, posMaxFeaturesMatched, maxFeaturesMatched,
                common.GetCurrentDateTimeStringWithMilliseconds())

        print >>fOutput, myText
        fOutput.flush()
        #posMaxFeaturesMatched = counterR

        counterRBest = posMaxFeaturesMatched
    else:
        # We empty the memoization cache before SimAnneal.main()
        SimAnneal.Acache = {}

        res = SimAnneal.main()
        res2 = (res[0], -res[1])

        common.DebugPrint(
            "Best solution for frame counterQ=%d is %s. Time = %s" % \
            (counterQ, str(res2), GetCurrentDateTimeStringWithMilliseconds()) )

        #!!!!TODO: check if OK:
        counterRBest = res[0]

    if COMPUTE_BEST_FAST == True:
        """
        !!!!TODO: check that these assignments really refer the objects defined above (when updating) and that there are no escapes of the values/objects that result in side-effects updating the respective objects and messing everything up - better said we look if redefinitons of the rhs are done inside (some of their subelements) or totally (reassign a completely NEW object).
        """
        p1 = p1Best
        p2 = p2Best
        kp_pairs = kp_pairsBest
        status = statusBest
        H = HBest
        nonp1 = nonp1Best

        Rest("Image Match")

        visBest = vis.copy()
        visOrigBest = visOrig.copy()

    if config.SAVE_FRAMES:
        if False:
            # We move the existing image pair of the matched frames to a special folder
            srcFileName = "img_proc_%05d_%05d.png" % (counterQ, counterRBest)
            dstFileName = config.FRAME_PAIRS_MATCHES_FOLDER + "/" + srcFileName
            srcFileName = config.FRAME_PAIRS_FOLDER + "/" + srcFileName

            try:
                shutil.move(srcFileName, dstFileName)
            except shutil.Error as exc:
                pass

        """
        We display frames imgQ and imgR with features (from temporal) and
            clusters on them.
        """
        """
        visBestCV = cv.fromarray(visBest)
        cv2.imwrite(config.FRAME_PAIRS_MATCHES_FOLDER + \
                        "/img_proc_%05d_%05d.png" % \
                        (counterQ, counterRBest), visBestCV)
        """
        cv2.imwrite(config.FRAME_PAIRS_MATCHES_FOLDER + \
                        "/img_proc_%05d_%05d.png" % \
                        (counterQ, counterRBest), visBest)

        # We display also the orig frames imgQ and imgR
        """
        visOrigBestCV = cv.fromarray(visOrigBest)
        cv2.imwrite(config.FRAME_PAIRS_MATCHES_FOLDER + \
                        "/img_proc_%05d_%05d_orig.png" % \
                        (counterQ, counterRBest), visOrigBestCV)
        """
        cv2.imwrite(config.FRAME_PAIRS_MATCHES_FOLDER + \
                        "/img_proc_%05d_%05d_orig.png" % \
                        (counterQ, counterRBest), visOrigBest)

    if False:
        #return (frame, ref_frame, feature_set, ref_feature_set)
        return (frameQ, posMaxFeaturesMatched, kp2[counterR], kp1[counterQ])

    """
    We return the len of the status returned by cv2.findHomography() - this is
        THE indicator of the match of a pair of frames.
    NOTE: we don't use the result :)))
    !!!!TODO: think to take it out
    """
    return res[1] #res

###############################################################################
###############################################################################
###############################################################################
#########################END TEMPORAL ALIGNMENT################################
###############################################################################
###############################################################################
###############################################################################

def DoTheRestAfterTemporalAlignment():
    global p1, p2, kp_pairs, status, H, nonp1
    #!!!!TODO: the *Best vars below need to be communicated either via return or made global

    p1 = p1Best
    p2 = p2Best
    kp_pairs = kp_pairsBest
    status = statusBest
    H = HBest
    nonp1 = nonp1Best

    Rest("Image Match")



def ProcessInputFrames(captureQ, captureR, fOutput):
    # Allocate numFeaturesMatched
    numFeaturesMatched = [None] * numFramesQ;
    for i in range(numFramesQ):
        numFeaturesMatched[i] = [-2000000000] * numFramesR;

    while True:
        if config.OCV_OLD_PY_BINDINGS:
            frameQ = captureQ.get(cv2.cv.CV_CAP_PROP_POS_FRAMES);
        else:
            frameQ = captureQ.get(cv2.CAP_PROP_POS_FRAMES);
        common.DebugPrint("Alex: frameQ = %d" % frameQ);

        counterQ = int(frameQ); #0
        common.DebugPrint("Alex: counterQ = %d" % counterQ);

        ret1, imgQ = captureQ.read();

        if False and config.SAVE_FRAMES:
            fileName = config.IMAGES_FOLDER + "/imgQ_%05d.png" % counterQ;
            if not os.path.exists(fileName):
                #print "dir(imgQ) = %s"% str(dir(imgQ))
                """
                imgQCV = cv.fromarray(imgQ)
                cv2.imwrite(fileName, imgQCV)
                """
                cv2.imwrite(fileName, imgQ);

        #if ret1 == False: #MatchFrames.counterQ == 3:
        if (ret1 == False) or (counterQ > numFramesQ):
            break;

        """
        I don't need to change to gray image if I do NOT do
            explore_match() , which requires gray to
            concatenate the 2 frames together.
        """
        #if True:
        if False:
            #common.ConvertImgToGrayscale(imgQ)
            #gray1 = common.ConvertImgToGrayscale(imgQ)
            imgQ = common.ConvertImgToGrayscale(imgQ);

        ComputeFeatures1(imgQ, counterQ); #!!!!TODO: counterQ already visible in module MatchFrames

        # We set the video stream captureR at the beginning
        if config.OCV_OLD_PY_BINDINGS:
            captureR.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, 0); #900)
        else:
            captureR.set(cv2.CAP_PROP_POS_FRAMES, 0); #900)

        # Start time profiling for the inner loop
        t1 = float(cv2.getTickCount());

        #!!!!TODO: counterQ already visible in module MatchFrames
        TemporalAlignment(counterQ, frameQ, captureR, \
                                numFramesR, numFeaturesMatched, fOutput);

        # Measuring how much it takes the inner loop
        t2 = float(cv2.getTickCount());
        myTime = (t2 - t1) / cv2.getTickFrequency();
        common.DebugPrint(
            "Avg time it takes to complete a match (and to perform " \
            "INITIAL Feat-Extract) = %.6f [sec]" % \
            (myTime / (numFramesR / config.counterRStep)) );

        counterQ += config.counterQStep;
        # If we try to seek to a frame out-of-bounds frame it gets to the last one
        if config.OCV_OLD_PY_BINDINGS:
            captureQ.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, counterQ);
        else:
            captureQ.set(cv2.CAP_PROP_POS_FRAMES, counterQ);

    common.DebugPrint("numFeaturesMatched = %s" % str(numFeaturesMatched));


if __name__ == '__main__':
    PreMain(nFramesQ=1000, nFramesR=1000);
    ComputeFeaturesAndMatch2(None, None);

